# Evaluation Results Summary (Example)

This file shows an example of how I might summarize LLM evaluation results
after running multiple prompt experiments.

## Summary Table

| Prompt / Use Case                       | Accuracy | Clarity | Grade Fit | Safety | Notes                             |
|----------------------------------------|----------|---------|-----------|--------|-----------------------------------|
| Math word problem (v1)                 | 4/5      | 5/5     | 5/5       | 5/5    | Solid; could be more engaging     |
| Reading passage + questions (v1)       | 5/5      | 4/5     | 4/5       | 5/5    | Some sentences slightly complex   |
| Mandarin vocabulary repetition (v1)    | 5/5      | 5/5     | 5/5       | 5/5    | Great for ages 6â€“9                |

## How I Would Use This

- Track which prompt versions perform best over time  
- Identify patterns in errors (e.g., clarity vs accuracy)  
- Communicate findings to product managers, data scientists, and engineers  
- Prioritize which prompts or use cases need further improvement
