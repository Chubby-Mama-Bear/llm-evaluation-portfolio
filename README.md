# LLM Evaluation Portfolio – Yingxin (Chloe) Li

A collection of LLM evaluation examples, prompt experiments, and educator-aligned rubrics.

This repository contains examples of my work evaluating large language model (LLM) outputs,
designing educator-aligned rubrics, testing prompt strategies, and analyzing model reliability.

## Background & Experience

- LLM reasoning evaluation (Outlier AI)
- Early elementary homeschool curriculum design (ages 6–9)
- Educational app review and instructional material design
- Error analysis, hallucination detection, and structured prompt experiments
- Workflow logic analysis and documentation

## Repository Structure

- `prompt-experiments/` – Sample prompt tests and LLM evaluation notes  
- `evaluation-rubrics/` – Rubrics for K–5 content quality, reasoning, and safety  
- `dashboards-examples/` – Example structures for tracking evaluation metrics over time  

## What This Portfolio Demonstrates

1. Prompt experiment design  
2. K–5 curriculum-aligned evaluation  
3. Rubrics for reasoning, safety, and clarity  
4. Example dashboards for tracking quality metrics  

These examples mirror techniques used in LLM quality analysis roles, especially in EdTech environments.
They are illustrative of my evaluation approach rather than logs from a production system.
