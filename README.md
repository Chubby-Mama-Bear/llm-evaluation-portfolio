# LLM Evaluation Portfolio – Yingxin (Chloe) Li

A collection of LLM evaluation examples, prompt experiments, and educator-aligned rubrics.

This repository contains examples of my work evaluating large language model (LLM) outputs,
designing educator-aligned rubrics, testing prompt strategies, and analyzing model reliability.

## Background & Experience
- LLM reasoning evaluation (Outlier AI)
- Early elementary homeschool curriculum design (ages 6–9)
- Educational app review and instructional material design
- Error analysis, hallucination detection, and structured prompt experiments
- Workflow logic analysis and documentation

## What This Portfolio Demonstrates
1. Prompt experiment design  
2. K–5 curriculum-aligned evaluation  
3. Rubrics for reasoning, safety, and clarity  
4. Example dashboards for tracking quality metrics  

These examples mirror the techniques used in LLM quality analysis roles, especially in EdTech environments.
